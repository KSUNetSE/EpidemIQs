
# config.yaml
query: "I am providing time-series data from the famous 1978 English boarding school influenza A/H1N1 outbreak. This dataset is known as an ``epidemic enigma'' because standard SEIR models often fail to fit the data while correctly predicting the final attack rate (AR).Parameters and constraints: the total population is $N = 763$ students. The observed attack rate is 512 students (67\%) who were eventually infected. Data interpretation: the data columns in the Excel file \texttt{cases\_data.xlsx} represent time, ``Confined to Bed'' ($B$), and ``Convalescent'' ($C$) counts. Crucial note: do not treat ``Confined to Bed'' ($B$) as the primary infectious stage $I$. The paper suggests an unobserved, highly infectious stage $I$ precedes the ``Confined to Bed'' stage $B$. The $B$ state acts effectively as a quarantine state where infectivity drops or stops. Virus characteristics: influenza A/H1N1. The generation time is likely short (approximately 1.9 days), and the basic reproduction number $R_0$ is estimated to be high (possibly $> 8$) due to the closed environment. Your task: analyze the provided \texttt{cases\_data.xlsx} file to extract the daily prevalence of students confined to bed and convalescing. Fit a compartmental model (or an agent-based equivalent) to this data. You must prioritize matching two objectives at the same time: the shape of the ``Confined to Bed'' curve and the total attack rate of about 512 students (standard models often overshoot to 100\% infection or fail to match the curve)"
name: "flue-1978"
workflow:
  copilot: False # whether to use copilot mode or not
  scientist_modules: # which modules to use in the scientist workflow, for reasoning models like openai's o3, planing might not be necessary
    plan: True
    reflect: True 
  reflection_max_iters: 1 # how many times the scientist can reflect (the higher is not necessarily better, start from lower values and increase if needed)
  no_retries: 5 # no  of scientist retries till generate the structured output
  no_paper_revise: 0 # no of times the scientist can revise the paper, best to test it with 0, if needed increase it to higher values
  time_out_tools: 600  # default timeout for tool execution in seconds
llm:
  scientists:
    provider: openai          # one of: openai, grok
    model: gpt-4.1   # model name

  experts:
    provider: openai
    model: gpt-4.1-mini

  mathematician_expert:
    provider: openai
    model: o3-mini

  vision_expert:            # make sure the model supports vision inputs!!!
    provider: openai
    model: gpt-4.1-mini

paths:
  output_dir: "user-files"
  contact_network_path: "NONE"
  data_path: "questions/cases_data.xlsx"
